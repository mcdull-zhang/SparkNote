### 1.InferFiltersFromGenerate导致的错误和重复计算

```sql
select explode(sequence(90, id, 1)) from dongdong_person where id>=90 and parse_url(name, 'a') is not null
```

原因：

```tex
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate ===
oldPlan:
Generate explode(sequence(90, id#11, Some(1), Some(Asia/Shanghai))), [0], false, [col#13]
+- Project [id#11]
   +- Filter ((id#11 >= 90) AND isnotnull(parse_url(name#12, a, false)))
      +- HiveTableRelation [`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11, name#12], Partition Cols: []]


newPlan:
Generate explode(sequence(90, id#11, Some(1), Some(Asia/Shanghai))), [0], false, [col#13]
+- Filter ((size(sequence(90, id#11, Some(1), Some(Asia/Shanghai)), true) > 0) AND isnotnull(sequence(90, id#11, Some(1), Some(Asia/Shanghai))))
   +- Project [id#11]
      +- Filter ((id#11 >= 90) AND isnotnull(parse_url(name#12, a, false)))
         +- HiveTableRelation [`default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#11, name#12], Partition Cols: []]
```



### 2.Hive表不支持动态分区裁剪

```sql
select 
  count(id) 
from 
  t1 
where 
  dt in (select dt from t2 where id=1)
```

上面的SQL被RewritePredicateSubquery优化后，会变为

```tex
Join LeftSemi, (dt#3 = dt#6)
:- Relation default.t1[id#1,name#2,dt#3] orc
+- Project [dt#6]
  +- Filter (isnotnull(id#4) AND (id#4 = 1))
      +- HiveTableRelation [`default`.`t2`, Data Cols: [id#4, name#5], Partition Cols: [dt#6]]
```



### 3.CollapseProject导致的表达式重复计算

```sql
select 
	temp['key1'],
	temp['key2'],
	temp['key3']
from (
  select str_to_map(str, ',', ':') temp from t1
)
```

会被转换成

```sql
select 
  str_to_map(str, ',', ':')['key1'],
  str_to_map(str, ',', ':')['key2'],
  str_to_map(str, ',', ':')['key3']
from 
  t1
```



### 4.UDF不支持序列化

Spark会在Driver端创建和初始化UDF对象，然后将UDF序列化给Executor来执行。

原因：https://github.com/apache/spark/pull/3640



### 5.OrderBy全局排序大量消耗driver内存

该PR对此问题进行了修复：https://github.com/apache/spark/pull/22961



### 6.Add HDFS jar导致的NPE

该PR对此问题进行了修复：https://github.com/apache/spark/pull/36301



### 7.row_number窗口函数必须指定order by字段

```tex
Caused by: org.apache.spark.sql.AnalysisException: Window function row_number() requires window to be ordered, please add ORDER BY clause. 
For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table;
```

Hive是支持这种语法的，如果不指定order by，则设置partition expression为order spec。



### 8.InferFiltersFromConstraints推断约束导致UDF执行失败

```sql
select 
  a.uid,a.item_id,b.loc
from
(
  select 
    uid, item_id
  from 
    t1
  where
    myUDF(item_id) > '2023'
) a
join
(
  select 
    loc, item_id
  from 
    t2
) b
on a.item_id=b.item_id;
```

由于Join的条件是`a.item_id=b.item_id`，那么认为`a.item_id`和`b.item_id`是等价，子查询a要求`myUDF(a.item_id) > '2023'`，那么可以推断出子查询b的`item_id`如果能join上子查询a的`item_id`，也必然满足`myUDF(b.item_id) > '2023'`，所以Spark就会将这个`myUDF(item_id) > '2023'`条件推测给子查询b来执行。



### 9.有状态的UDF的下推

```sql
select 
  a.uid,a.item_id,b.loc
from
(
  select 
    uid, item_id
  from 
    t1
) a
join
(
  select 
    loc, item_id
  from 
    t2
) b
on a.item_id=b.item_id
where myUDF(uid)=1;
```

在Hive0.13中myUDF并不会下推，也就是执行完join之后才会进行myUDF的过滤。



### 10.特殊的limit

以下SQL在Spark和Hive2.3.7中不能执行，但是在Hive0.13中可以执行：

```sql
SELECT A FROM TABLE_1 LIMIT 1
UNION 
SELECT A FROM TABLE_2 LIMIT 2
```

spark社区目前不会修复该问题：https://issues.apache.org/jira/browse/SPARK-28327



### 11.groupBy数字常量

```sql
select 20230614,count(1) from t1 group by 20230614;
```

上面的sql在Hive中可以正常执行，但是在Spark中会报错：

```tex
org.apache.spark.sql.AnalysisException: GROUP BY position 20230614 is not in select list
```



### 12.Spark不支持批量删除分区

```sql
alter table t drop partition(dt <= '20230613')
```



### 13.模糊列识别

```sql
select 
  id, name 
from 
  t1 
join 
  t2 
on 
	t1.id=t2.id 
group by 
	t1.id,t1.name
```

在Hive中可以运行，在Spark中报错：

```tex
org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: t1.id, t2.id.;
```



### 14.UDF返回自定类型

Spark只允许UDF返回基本数据类型、Array、Map、Struct或者其Writable类型，否则报错：

```tex
Caused by: org.apache.spark.sql.AnalysisException: No handler for UDF/UDAF/UDTF '${udf类名}': org.apache.spark.sql.AnalysisException: Unsupported java type class ${udf返回值类名};
```











